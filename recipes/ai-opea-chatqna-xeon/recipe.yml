# Ansible Playbook of the recipe found here: https://github.com/opea-project/GenAIExamples/blob/main/ChatQnA/microservice/gaudi/README.md
# This is a simple example of a microservice that uses the Gaudi framework to create a simple chatbot that answers questions.
# Needs to be run as root user or with sudo privileges.
# To use the newer version of the example, see commented out sections below
---
- name: OPEA ChatQnA
  hosts: localhost
  tasks:
    - name: Install pre-requisite packages
      ansible.builtin.apt:
        pkg:
          - apt-transport-https
          - ca-certificates
          - curl
          - gnupg
          - lsb-release
          - software-properties-common
        state: present
        update_cache: true
    - name: Add Docker's GPG key to system
      shell: |
        install -m 0755 -d /etc/apt/keyrings
        curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
        chmod a+r /etc/apt/keyrings/docker.asc
    - name: Add Docker repository to apt sources
      shell: |
        echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null
    - name: Run the equivalent of "apt-get update" as a separate step
      ansible.builtin.apt:
        update_cache: yes
    - name: Install Docker
      apt:
        name:
          - docker-ce
          - docker-ce-cli
          - containerd.io
          - docker-buildx-plugin
          - docker-compose-plugin
    - name: Clone the GenAIComps repo
      git:
        repo: https://github.com/opea-project/GenAIComps.git
        dest: /opt/GenAIComps
    - name: Clone the GenAIExamples repo
      git:
        repo: https://github.com/lucasmelogithub/GenAIExamples.git #To be updated to opea-project org later per Lucas Melo
        dest: /opt/GenAIExamples
    # Uncomment to use the newer version of the example
    # - name: Clone the vllm repo
    #   git:
    #     repo: https://github.com/vllm-project/vllm.git
    #     dest: /opt/vllm
    - name: Pull Embedding Image
      ansible.builtin.shell:
        cmd: |
          docker pull opea/embedding-tei:latest
    - name: Pull Retriever Image
      ansible.builtin.shell:
        cmd: |
          docker pull opea/retriever-redis:latest
    - name: Pull Rerank Image
      ansible.builtin.shell:
        cmd: |
          docker pull opea/reranking-tei:latest
    - name: Pull LLM Image
      ansible.builtin.shell:
        cmd: |
          docker pull opea/llm-tgi:latest
    - name: Pull Dataprep Image
      ansible.builtin.shell:
        cmd: |
          docker pull opea/dataprep-redis:latest
    - name: Pull ChatQnA MegaService Docker Image
      ansible.builtin.shell:
        cmd: |
          docker pull opea/chatqna:latest
    - name: Pull ChatQnA UI Docker Image
      ansible.builtin.shell:
        cmd: |
          docker pull opea/chatqna-ui:latest
    ## Uncomment to use the newer version of the example
    # - name: Pull UI Converstation Image
    #   ansible.builtin.shell:
    #     cmd: |
    #       docker pull opea/chatqna-conversation-ui:latest 
    # - name: Build vllm:cpu image
    #   ansible.builtin.shell:
    #     cmd: |
    #       cd /opt/vllm
    #       docker build -t vllm:cpu -f /opt/vllm/Dockerfile.cpu .
    - name: Launch the OPEA containers
      ansible.builtin.shell:
        cmd: |
          . /etc/profile.d/opea.sh
          cd /opt/GenAIExamples/ChatQnA/docker/xeon
          docker compose -f compose.yaml up -d
    ## For the newer version of the example, replace the line above with the line below
          # docker compose -f docker_compose_vllm.yaml up -d
